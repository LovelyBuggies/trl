# Model training configuration for GRPO

# Compression settings
compression: "low"  # Options: "high" (4-bit), "medium" (8-bit), or "low" (16-bit)

# Model settings
model_name: "Qwen/Qwen2.5-0.5B"
target_modules: 
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"

# Dataset settings
dataset: "trl-lib/tldr"
dataset_split: "train[:20000]"

# Training parameters
batch_size: 2
gradient_accumulation_steps: 8
learning_rate: 1e-5
num_train_epochs: 3
warmup_steps: 100
max_grad_norm: 0.3

# GRPO specific parameters
max_prompt_length: 256
max_completion_length: 128
num_generations: 4  # Number of generations per prompt (group size)
temperature: 1.0
top_p: 0.9
top_k: 50

# Reward function settings
use_length_reward: true
use_capitalization_reward: true
reward_weights: [0.3, 0.7]  # Weights for the reward functions (length, capitalization)

# Output settings
output_dir: "./results"
save_dir: "qwen-0.5b-grpo-low-duel-rewards"
logging_dir: "./logs"
logging_steps: 10
save_steps: 200
save_total_limit: 3

# Weights & Biases integration
use_wandb: true
wandb_project: "trl"
wandb_entity: "nu-llpr"  # Your Weights & Biases username or organization
run_name: "qwen-0.5b-grpo-low-duel-rewards"
