from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.trainer_callback import TrainerCallback
from peft import LoraConfig, get_peft_model
from trl import GRPOConfig, GRPOTrainer
import argparse
import yaml
import numpy as np
import os
from tqdm import tqdm
import torch
from sklearn.metrics import accuracy_score


def parse_arguments():
    """Parse command line arguments for configuration settings."""
    parser = argparse.ArgumentParser(
        description="Fine-tune a language model with GRPO on MMLU dataset."
    )
    parser.add_argument(
        "--config",
        type=str,
        help="Path to YAML configuration file"
    )
    return parser.parse_args()


def load_config(config_file):
    """Load configuration from YAML file."""
    try:
        with open(config_file, 'r') as stream:
            return yaml.safe_load(stream)
    except yaml.YAMLError as exc:
        print(f"Error in configuration file: {exc}")
        return {}
    except FileNotFoundError:
        print(f"Configuration file not found: {config_file}")
        return {}


def reward_correct_answer(completions, choices, answer, **kwargs):
    """
    Reward function that gives +10 points if the response contains the correct answer,
    and -4 points otherwise.

    This function examines each completion to determine if it indicates the correct answer.
    It converts the answer letter (A, B, C, D) to patterns to look for in the completion.

    Args:
        completions (list): List of text completions generated by the model
        choices (list): List of choice options for the question
        answer (str or list): The correct answer (usually a letter like "A", "B", "C", or "D")
                              Can be a list or a string depending on dataset format

    Returns:
        list: List of reward scores corresponding to each completion
    """
    rewards = []

    # Handle answer as either a string or a list
    if isinstance(answer, list):
        if len(answer) > 0:
            correct_letter = answer[0]  # Use the first element if it's a list
        else:
            correct_letter = "A"  # Default if empty list (shouldn't happen)
    else:
        correct_letter = answer

    # Ensure correct_letter is a string
    correct_letter = str(correct_letter)

    for completion in completions:
        # Convert to lowercase for case-insensitive matching
        text = completion.lower()

        # Check if completion contains the correct answer
        contains_correct = False

        # Pattern to look for based on the correct answer
        patterns = [
            f"answer is {correct_letter.lower()}",
            f"answer: {correct_letter.lower()}",
            f"answer {correct_letter.lower()}",
            f"option {correct_letter.lower()}",
            f"choice {correct_letter.lower()}",
            f"select {correct_letter.lower()}",
            f"choose {correct_letter.lower()}",
            f"{correct_letter.lower()} is correct",
            f"pick {correct_letter.lower()}",
            f"the answer is {correct_letter.lower()}",
            f"i choose {correct_letter.lower()}",
            f"i select {correct_letter.lower()}"
        ]

        # Check each pattern
        for pattern in patterns:
            if pattern in text:
                contains_correct = True
                break

        # Assign reward based on whether the answer is correct
        if contains_correct:
            rewards.append(10.0)  # High reward for correct answer
        else:
            rewards.append(-4.0)  # Penalty for incorrect answer

    return rewards


def prepare_mmlu_dataset(dataset_name="tasksource/mmlu", split="dev", subset="abstract_algebra", max_samples=None):
    """
    Prepare MMLU dataset for GRPO training.

    Args:
        dataset_name (str): Name of the dataset on Hugging Face.
        split (str): Split to use (dev or test).
        subset (str): MMLU subject to use (or "all" for all subjects).
        max_samples (int, optional): Maximum number of samples to use.

    Returns:
        Dataset: Processed dataset ready for training.
    """
    # Load the dataset
    if subset == "all":
        from datasets import concatenate_datasets
        print("Loading all MMLU subsets...")
        subset_list = [
            "abstract_algebra", "anatomy", "astronomy", "business_ethics",
            "clinical_knowledge", "college_biology", "college_chemistry", "college_computer_science",
            "college_mathematics", "college_physics", "computer_security", "conceptual_physics",
            "econometrics", "electrical_engineering", "elementary_mathematics", "formal_logic",
            "global_facts", "high_school_biology", "high_school_chemistry", "high_school_computer_science",
            "high_school_european_history", "high_school_geography", "high_school_government_and_politics",
            "high_school_macroeconomics", "high_school_mathematics", "high_school_microeconomics",
            "high_school_physics", "high_school_psychology", "high_school_statistics",
            "high_school_us_history", "high_school_world_history", "human_sexuality", "international_law",
            "jurisprudence", "logical_fallacies", "machine_learning", "management", "marketing",
            "medical_genetics", "miscellaneous", "moral_disputes", "moral_scenarios", "nutrition",
            "philosophy", "prehistory", "professional_accounting", "professional_law", "professional_medicine",
            "professional_psychology", "public_relations", "security_studies", "sociology",
            "us_foreign_policy", "virology", "world_religions"
        ]
        datasets = [load_dataset(dataset_name, s, split=split) for s in subset_list]
        dataset = concatenate_datasets(datasets)
        print(f"Loaded {len(dataset)} total examples from all subsets.")
    else:
        dataset = load_dataset(dataset_name, subset, split=split)

    # Limit the number of samples if specified
    if max_samples is not None and max_samples < len(dataset):
        dataset = dataset.select(range(max_samples))

    # Format the dataset for GRPO training
    def format_example(example):
        # Create a prompt in the format expected by the model
        prompt = f"Question: {example['question']}\n"
        prompt += "Options:\n"

        # Add options A, B, C, D
        options = example['choices']
        for i, option in enumerate(options):
            option_letter = chr(65 + i)  # Convert 0->A, 1->B, etc.
            prompt += f"{option_letter}. {option}\n"

        prompt += "Answer:"

        # Keep choices and answer for the reward function
        return {
            "prompt": prompt,
            "choices": example['choices'],
            "answer": example['answer'],  # This is the correct answer letter (A, B, C, or D)
            "subject": example.get('subject', subset)  # Add subject for more detailed tracking
        }

    # Apply the formatting to each example
    processed_dataset = dataset.map(format_example)

    return processed_dataset


def evaluate_mmlu(model, tokenizer, eval_dataset, device="cuda", batch_size=16, wandb_logger=None):
    """
    Evaluate model performance on MMLU dataset and log results to wandb.

    Args:
        model: The model to evaluate
        tokenizer: The tokenizer for the model
        eval_dataset: The evaluation dataset
        device: The device to run evaluation on
        batch_size: Batch size for evaluation
        wandb_logger: WandB logger instance

    Returns:
        dict: Dictionary containing evaluation metrics
    """
    model.eval()
    all_preds = []
    all_labels = []
    all_subjects = []

    # Dictionary to store per-subject metrics
    subject_results = {}

    # Create batches for evaluation
    for i in range(0, len(eval_dataset), batch_size):
        batch = eval_dataset[i:min(i + batch_size, len(eval_dataset))]
        batch_prompts = batch["prompt"]
        batch_answers = batch["answer"]
        batch_subjects = batch["subject"]

        # Track subjects
        all_subjects.extend(batch_subjects)

        # Tokenize inputs
        inputs = tokenizer(batch_prompts, return_tensors="pt", padding=True, truncation=True)

        # Move to device
        inputs = {k: v.to(device) for k, v in inputs.items()}

        # Generate completions
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=16,
                temperature=0.1,
                num_return_sequences=1,
                pad_token_id=tokenizer.eos_token_id
            )

        # Decode generated text
        generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)

        # Process each generation to extract the predicted answer
        for j, gen_text in enumerate(generated_texts):
            # Get the part after the prompt
            prompt = batch_prompts[j % len(batch_prompts)]
            completion = gen_text[len(prompt):]

            # Clean up the completion by taking only the first few characters
            # This handles cases like " A\n\n..." by limiting to first part of response
            first_line = completion.split('\n')[0] if '\n' in completion else completion
            clean_completion = first_line.strip()[:20]  # Take just first 20 chars max

            # Find the predicted letter
            predicted_idx = None

            # Method 1: Look for patterns like "answer is X" in the completion
            answer_patterns = ["answer is ", "answer: ", "answer ", "option ", "choice ",
                               "select ", "choose ", " is correct", "pick ", "the answer is ",
                               "i choose ", "i select "]

            for pattern in answer_patterns:
                if pattern in clean_completion.lower():
                    idx = clean_completion.lower().find(pattern) + len(pattern)
                    if idx < len(clean_completion):
                        potential_letter = clean_completion[idx].upper()
                        if potential_letter in "ABCD":
                            # Convert letter to index (A->0, B->1, C->2, D->3)
                            predicted_idx = ord(potential_letter) - ord('A')
                            break

            # Method 2: Look for standalone letters at the beginning of the completion
            if predicted_idx is None and clean_completion:
                # Check if the completion starts with a letter (possibly with space before)
                for first_char in clean_completion.strip():
                    if first_char.upper() in "ABCD":
                        predicted_idx = ord(first_char.upper()) - ord('A')
                        break

            # Method 3: Check if any ABCD appears anywhere in the first few tokens
            if predicted_idx is None:
                for letter in "ABCD":
                    if letter in clean_completion.upper():
                        predicted_idx = ord(letter) - ord('A')
                        break

            # Default to A (index 0) if no prediction found
            if predicted_idx is None:
                predicted_idx = 0

            # Get the correct answer (already as index 0,1,2,3)
            correct_idx = batch_answers[j % len(batch_answers)]

            # Add to overall results (keeping everything as indices)
            all_preds.append(predicted_idx)
            all_labels.append(correct_idx)

    # Calculate overall accuracy manually since we're using indices
    correct = sum(1 for p, l in zip(all_preds, all_labels) if p == l)
    total = len(all_preds)
    accuracy = correct / total if total > 0 else 0

    # Calculate per-subject accuracy
    for subject in set(all_subjects):
        subject_indices = [i for i, s in enumerate(all_subjects) if s == subject]
        subject_preds = [all_preds[i] for i in subject_indices]
        subject_labels = [all_labels[i] for i in subject_indices]

        # Calculate accuracy manually
        subject_correct = sum(1 for p, l in zip(subject_preds, subject_labels) if p == l)
        subject_total = len(subject_indices)
        subject_acc = subject_correct / subject_total if subject_total > 0 else 0

        subject_results[subject] = {
            "accuracy": subject_acc,
            "count": subject_total
        }

    # Sort subjects by accuracy
    sorted_subjects = sorted(subject_results.items(), key=lambda x: x[1]["accuracy"], reverse=True)

    # Log results to wandb if available
    if wandb_logger:
        # Log overall metrics
        wandb_logger.log({"eval/accuracy": accuracy})

        # Log per-subject accuracy (only top and bottom 5 to avoid cluttering)
        top_subjects = sorted_subjects[:5]
        bottom_subjects = sorted_subjects[-5:] if len(sorted_subjects) > 5 else []

        for subject, metrics in dict(top_subjects + bottom_subjects).items():
            wandb_logger.log({f"eval/subject/{subject}/accuracy": metrics["accuracy"]})

    return {
        "accuracy": accuracy,
        "subject_results": subject_results
    }


class EvaluationCallback(TrainerCallback):
    """
    Callback for evaluating the model at specified intervals during training.
    Inherits from TrainerCallback to properly integrate with the HuggingFace Trainer.
    """
    def __init__(self, eval_function, eval_interval=100):
        self.eval_function = eval_function
        self.eval_interval = eval_interval
        self.last_eval_step = 0

    def on_step_end(self, args, state, control, **kwargs):
        """Called at the end of a training step."""
        # Check if it's time to evaluate
        if state.global_step - self.last_eval_step >= self.eval_interval:
            self.last_eval_step = state.global_step
            # Manually call evaluation function
            self.eval_function()
        return control

class CustomGRPOTrainer(GRPOTrainer):
    """Extended GRPO Trainer with additional functionality for WandB logging and evaluation."""

    def __init__(self, model, args, train_dataset, eval_dataset=None, tokenizer=None,
                 reward_funcs=None, wandb_logger=None, **kwargs):
        super().__init__(model=model, args=args, train_dataset=train_dataset,
                         reward_funcs=reward_funcs, **kwargs)
        self.eval_dataset = eval_dataset
        self.tokenizer = tokenizer
        self.wandb_logger = wandb_logger
        self.best_accuracy = 0.0
        self.last_log_step = 0

    def training_step(self, *args, **kwargs):
        """Override training step to add custom logging."""
        loss = super().training_step(*args, **kwargs)

        # Log detailed training metrics if wandb is enabled - but not too frequently
        # Only log if this step is a multiple of logging_steps and we haven't logged recently
        current_step = self.state.global_step
        if (self.wandb_logger and
                current_step % self.args.logging_steps == 0 and
                current_step > self.last_log_step):

            self.last_log_step = current_step

            # Basic metrics
            metrics = {
                "train/loss": loss.item(),
                "train/learning_rate": self.lr_scheduler.get_last_lr()[0],
            }

            # Add reward stats if available
            if hasattr(self, "reward_stats") and len(self.reward_stats) > 0:
                metrics["train/mean_reward"] = np.mean(self.reward_stats)

            self.wandb_logger.log(metrics)

        return loss

    def evaluate(self, eval_dataset=None):
        """Run evaluation and log results to wandb."""
        if eval_dataset is None:
            eval_dataset = self.eval_dataset

        if eval_dataset is None:
            return {}

        print("\nRunning evaluation...")
        results = evaluate_mmlu(
            model=self.model,
            tokenizer=self.tokenizer,
            eval_dataset=eval_dataset,
            device=self.args.device,
            wandb_logger=self.wandb_logger
        )

        # Save the best model
        if results["accuracy"] > self.best_accuracy:
            self.best_accuracy = results["accuracy"]
            self.save_model(os.path.join(self.args.output_dir, "best_model"))

            if self.wandb_logger:
                self.wandb_logger.log({"eval/best_accuracy": self.best_accuracy})

        print(f"Evaluation accuracy: {results['accuracy']:.4f}")
        return results


def main():
    # Parse command line arguments
    args = parse_arguments()

    # Load config from YAML file if provided
    config = {}
    if args.config:
        config = load_config(args.config)

    # Get other parameters from config
    use_wandb = config.get('use_wandb', False)
    wandb_project = config.get('wandb_project', 'trl')
    wandb_entity = config.get('wandb_entity', 'entity')
    run_name = config.get('run_name', f'grpo-mmlu')

    # Initialize wandb logger
    wandb_logger = None
    if use_wandb:
        # Initialize wandb if enabled
        try:
            import wandb
            # Just use the config directly without creating a separate dict
            wandb_logger = wandb.init(
                project=wandb_project,
                entity=wandb_entity,
                name=run_name,
                config=config  # Pass the entire config to wandb
            )
        except ImportError:
            print("Warning: wandb not installed. Running without Weights & Biases tracking.")
            use_wandb = False

    # Load the MMLU dataset
    mmlu_subset = config.get('mmlu_subset', 'all')
    mmlu_split = config.get('mmlu_split', 'dev')
    max_samples = config.get('max_samples', 100)

    train_dataset = prepare_mmlu_dataset(
        dataset_name=config.get('dataset', "tasksource/mmlu"),
        split=mmlu_split,
        subset=mmlu_subset,
        max_samples=max_samples
    )

    # Prepare evaluation dataset
    eval_dataset = None
    if config.get('eval_split'):
        eval_split = config.get('eval_split', 'test')
        eval_max_samples = config.get('eval_max_samples', 200)

        eval_dataset = prepare_mmlu_dataset(
            dataset_name=config.get('dataset', "tasksource/mmlu"),
            split=eval_split,
            subset=mmlu_subset,
            max_samples=eval_max_samples
        )

        print(f"Loaded {len(train_dataset)} examples for training and {len(eval_dataset)} examples for evaluation")
    else:
        print(f"Loaded {len(train_dataset)} examples for training (no evaluation dataset)")

    # We can use more expressive LoRA settings
    lora_r = config.get('lora_r', 32)
    lora_alpha = config.get('lora_alpha', 64)

    # Load the model with appropriate quantization
    model_name = config.get('model_name', "Qwen/Qwen2.5-0.5B")
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",
        trust_remote_code=True  # Needed for some models like Qwen
    )

    # Load tokenizer for evaluation
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True,
        padding_side="left"  # Set padding side to left for decoder-only models
    )

    # Make sure the tokenizer has the padding token properly set
    if tokenizer.pad_token is None:
        # For models like Llama, GPT, etc. that don't have a pad token
        tokenizer.pad_token = tokenizer.eos_token

    # Configure LoRA - adjust target modules as needed for your specific model
    target_modules = config.get('target_modules', ["q_proj", "k_proj", "v_proj", "o_proj"])

    peft_config = LoraConfig(
        r=lora_r,
        lora_alpha=lora_alpha,
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=target_modules,
        # Additional memory optimizations:
        fan_in_fan_out=False,
        modules_to_save=None,
    )

    # Apply LoRA adapters
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()  # Print percentage of trainable parameters

    # Configure the GRPO trainer
    eval_steps = config.get('eval_steps', 100)
    training_args = GRPOConfig(
        output_dir=config.get('output_dir', './results'),
        num_train_epochs=config.get('num_train_epochs', 1),
        per_device_train_batch_size=config.get('batch_size', 2),
        gradient_accumulation_steps=config.get('gradient_accumulation_steps', 4),
        learning_rate=float(config.get('learning_rate', 1e-5)),
        max_grad_norm=float(config.get('max_grad_norm', 0.3)),
        warmup_steps=config.get('warmup_steps', 20),
        logging_dir=config.get('logging_dir', './logs'),
        logging_steps=config.get('logging_steps', 10),
        save_steps=config.get('save_steps', 100),
        save_total_limit=config.get('save_total_limit', 3),

        # GRPO specific parameters
        max_prompt_length=config.get('max_prompt_length', 128),
        max_completion_length=config.get('max_completion_length', 64),
        num_generations=config.get('num_generations', 2),
        temperature=config.get('temperature', 1.0),
        top_p=config.get('top_p', 0.9),

        # Reporting
        report_to="wandb" if use_wandb else "none",
        run_name=run_name if use_wandb else None,
    )

    # Initialize our custom GRPO trainer with wandb logging
    trainer = CustomGRPOTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        reward_funcs=reward_correct_answer,
        wandb_logger=wandb_logger
    )

    # Evaluate model before training if evaluation dataset is provided
    if eval_dataset is not None:
        print("Evaluating model before training...")
        trainer.evaluate()

    # Set up custom evaluation callback correctly
    if eval_dataset is not None:
        # Create a closure function that will call our trainer's evaluate method
        def evaluate_func():
            print(f"\nRunning evaluation at step {trainer.state.global_step}...")
            results = trainer.evaluate()
            print(f"Evaluation accuracy: {results.get('accuracy', 0):.4f}")

            # Save model if it's the best so far
            if hasattr(trainer, 'best_accuracy') and results.get('accuracy', 0) > trainer.best_accuracy:
                trainer.best_accuracy = results.get('accuracy', 0)
                save_path = os.path.join(training_args.output_dir, "best_model")
                trainer.save_model(save_path)
                print(f"New best model saved with accuracy: {trainer.best_accuracy:.4f}")

        # Create the callback with our evaluate function
        callback = EvaluationCallback(evaluate_func, eval_steps)
        trainer.add_callback(callback)

    # Start training
    trainer.train()

    # Final evaluation
    if eval_dataset is not None:
        print("Final evaluation after training...")
        trainer.evaluate()

    # Save the fine-tuned model
    output_dir = config.get('save_dir', "./final_model")
    trainer.save_model(output_dir)
    print(f"Model saved to {output_dir}")

    # Close wandb
    if wandb_logger:
        import wandb
        wandb.finish()


if __name__ == "__main__":
    main()