from datasets import load_dataset
from transformers import AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import GRPOConfig, GRPOTrainer
import torch
import argparse
import yaml
import os

def parse_arguments():
    """Parse command line arguments for configuration settings."""
    parser = argparse.ArgumentParser(
        description="Fine-tune a language model with GRPO on MMLU dataset."
    )
    parser.add_argument(
        "--config", 
        type=str,
        help="Path to YAML configuration file"
    )
    parser.add_argument(
        "--compression", 
        type=str, 
        default=None,
        choices=["high", "medium", "low"],
        help="Compression level: high (4-bit), medium (8-bit), or low (16-bit)"
    )
    return parser.parse_args()

def load_config(config_file):
    """Load configuration from YAML file."""
    try:
        with open(config_file, 'r') as stream:
            return yaml.safe_load(stream)
    except yaml.YAMLError as exc:
        print(f"Error in configuration file: {exc}")
        return {}
    except FileNotFoundError:
        print(f"Configuration file not found: {config_file}")
        return {}

def reward_correct_answer(completions, choices, answer, **kwargs):
    """
    Reward function that gives +10 points if the response contains the correct answer,
    and -4 points otherwise.
    
    This function examines each completion to determine if it indicates the correct answer.
    It converts the answer letter (A, B, C, D) to patterns to look for in the completion.
    
    Args:
        completions (list): List of text completions generated by the model
        choices (list): List of choice options for the question
        answer (str or list): The correct answer (usually a letter like "A", "B", "C", or "D")
                              Can be a list or a string depending on dataset format
        
    Returns:
        list: List of reward scores corresponding to each completion
    """
    rewards = []
    
    # Handle answer as either a string or a list
    if isinstance(answer, list):
        if len(answer) > 0:
            correct_letter = answer[0]  # Use the first element if it's a list
        else:
            correct_letter = "A"  # Default if empty list (shouldn't happen)
    else:
        correct_letter = answer
    
    # Ensure correct_letter is a string
    correct_letter = str(correct_letter)
    
    for completion in completions:
        # Convert to lowercase for case-insensitive matching
        text = completion.lower()
        
        # Check if completion contains the correct answer
        contains_correct = False
        
        # Pattern to look for based on the correct answer
        patterns = [
            f"answer is {correct_letter.lower()}",
            f"answer: {correct_letter.lower()}",
            f"answer {correct_letter.lower()}",
            f"option {correct_letter.lower()}",
            f"choice {correct_letter.lower()}",
            f"select {correct_letter.lower()}",
            f"choose {correct_letter.lower()}",
            f"{correct_letter.lower()} is correct",
            f"pick {correct_letter.lower()}",
            f"the answer is {correct_letter.lower()}",
            f"i choose {correct_letter.lower()}",
            f"i select {correct_letter.lower()}"
        ]
        
        # Check each pattern
        for pattern in patterns:
            if pattern in text:
                contains_correct = True
                break
                
        # Assign reward based on whether the answer is correct
        if contains_correct:
            rewards.append(10.0)  # High reward for correct answer
        else:
            rewards.append(-4.0)  # Penalty for incorrect answer
            
    return rewards

def prepare_mmlu_dataset(dataset_name="tasksource/mmlu", split="dev", subset="abstract_algebra", max_samples=None):
    """
    Prepare MMLU dataset for GRPO training.
    
    Args:
        dataset_name (str): Name of the dataset on Hugging Face.
        split (str): Split to use (dev or test).
        subset (str): MMLU subject to use (or "all" for all subjects).
        max_samples (int, optional): Maximum number of samples to use.
        
    Returns:
        Dataset: Processed dataset ready for training.
    """
    # Load the dataset
    if subset == "all":
        # For "all", we need to load a specific subset first, then potentially
        # load others and concatenate them
        print("Loading 'abstract_algebra' subset as a starting point...")
        dataset = load_dataset(dataset_name, "abstract_algebra", split=split)
        
        # Here you could load additional subsets and concatenate them
        # For simplicity, we're just using one subset for now
        print("Note: Currently only using 'abstract_algebra' subset.")
    else:
        # Load specific subject
        dataset = load_dataset(dataset_name, subset, split=split)
    
    # Limit the number of samples if specified
    if max_samples is not None and max_samples < len(dataset):
        dataset = dataset.select(range(max_samples))
    
    # Format the dataset for GRPO training
    def format_example(example):
        # Create a prompt in the format expected by the model
        prompt = f"Question: {example['question']}\n"
        prompt += "Options:\n"
        
        # Add options A, B, C, D
        options = example['choices']
        for i, option in enumerate(options):
            option_letter = chr(65 + i)  # Convert 0->A, 1->B, etc.
            prompt += f"{option_letter}. {option}\n"
        
        prompt += "Answer:"
        
        # Keep choices and answer for the reward function
        return {
            "prompt": prompt,
            "choices": example['choices'],
            "answer": example['answer']  # This is the correct answer letter (A, B, C, or D)
        }
    
    # Apply the formatting to each example
    processed_dataset = dataset.map(format_example)
    
    return processed_dataset

def main():
    # Parse command line arguments
    args = parse_arguments()
    
    # Load config from YAML file if provided
    config = {}
    if args.config:
        config = load_config(args.config)
    
    # Command line arguments override config file settings
    compression = args.compression or config.get('compression', 'high')
    
    # Get other parameters from config
    use_wandb = config.get('use_wandb', False)
    wandb_project = config.get('wandb_project', 'mmlu-grpo-training')
    wandb_entity = config.get('wandb_entity', None)
    run_name = config.get('run_name', f'mmlu-grpo-{compression}-compression')
    
    print(f"Using {compression} compression level")
    if use_wandb:
        # Initialize wandb if enabled
        if wandb_entity:
            print(f"Weights & Biases enabled for entity: {wandb_entity}")
        else:
            print(f"Weights & Biases enabled (default entity)")
        print(f"Project: {wandb_project}")
        print(f"Run name: {run_name}")
        
        # Import and initialize wandb here if it's enabled
        if use_wandb:
            try:
                import wandb
                wandb.init(
                    project=wandb_project,
                    entity=wandb_entity,
                    name=run_name,
                    config={
                        "compression": compression,
                        "model_name": config.get('model_name', "Qwen/Qwen2.5-0.5B"),
                        "learning_rate": config.get('learning_rate', 5e-5),
                        "epochs": config.get('num_train_epochs', 3),
                        "batch_size": config.get('batch_size', 4),
                    }
                )
            except ImportError:
                print("Warning: wandb not installed. Running without Weights & Biases tracking.")
                use_wandb = False
    
    # Load the MMLU dataset
    mmlu_subset = config.get('mmlu_subset', 'all')
    mmlu_split = config.get('mmlu_split', 'dev')
    max_samples = config.get('max_samples', 100)
    dataset = prepare_mmlu_dataset(
        dataset_name=config.get('dataset', "tasksource/mmlu"),
        split=mmlu_split,
        subset=mmlu_subset,
        max_samples=max_samples
    )
    
    print(f"Loaded {len(dataset)} examples from MMLU dataset")
    
    # Configure quantization based on compression level
    if compression == "high":
        print("Using high compression (4-bit quantization)")
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        # For high compression, we might want more aggressive LoRA settings
        lora_r = 8
        lora_alpha = 16
    elif compression == "medium":
        print("Using medium compression (8-bit quantization)")
        bnb_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0
        )
        # For medium compression, we can use standard LoRA settings
        lora_r = 16
        lora_alpha = 32
    else:  # "low" or any other value
        print("Using low compression (16-bit/half precision)")
        bnb_config = None
        # For low compression, we can use more expressive LoRA settings
        lora_r = 32
        lora_alpha = 64
    
    # Load the model with appropriate quantization
    model_name = config.get('model_name', "Qwen/Qwen2.5-0.5B")
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True  # Needed for some models like Qwen
    )
    
    # Prepare the model for k-bit training
    model = prepare_model_for_kbit_training(model)
    
    # Configure LoRA - adjust target modules as needed for your specific model
    target_modules = config.get('target_modules', ["q_proj", "k_proj", "v_proj", "o_proj"])
    
    peft_config = LoraConfig(
        r=lora_r,
        lora_alpha=lora_alpha,
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=target_modules,
        # Additional memory optimizations:
        fan_in_fan_out=False,
        modules_to_save=None,
    )
    
    # Apply LoRA adapters
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()  # Print percentage of trainable parameters
    
    # Configure the GRPO trainer
    training_args = GRPOConfig(
        output_dir=config.get('output_dir', './results'),
        num_train_epochs=config.get('num_train_epochs', 1),
        per_device_train_batch_size=config.get('batch_size', 2),
        gradient_accumulation_steps=config.get('gradient_accumulation_steps', 4),
        learning_rate=float(config.get('learning_rate', 1e-5)),
        max_grad_norm=float(config.get('max_grad_norm', 0.3)),
        warmup_steps=config.get('warmup_steps', 20),
        logging_dir=config.get('logging_dir', './logs'),
        logging_steps=config.get('logging_steps', 10),
        save_steps=config.get('save_steps', 100),
        save_total_limit=config.get('save_total_limit', 3),
        
        # GRPO specific parameters
        max_prompt_length=config.get('max_prompt_length', 128),
        max_completion_length=config.get('max_completion_length', 64),
        num_generations=config.get('num_generations', 2),
        temperature=config.get('temperature', 1.0),
        top_p=config.get('top_p', 0.9),
        
        # Reporting
        report_to="wandb" if use_wandb else "none",
        run_name=run_name if use_wandb else None,
    )
    
    # Initialize the GRPO trainer with our custom reward function
    trainer = GRPOTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        reward_funcs=reward_correct_answer,
    )
    
    # Start training
    trainer.train()
    
    # Save the fine-tuned model
    output_dir = config.get('save_dir', f"mmlu-grpo-{compression}-compression")
    trainer.save_model(output_dir)
    print(f"Model saved to {output_dir}")

if __name__ == "__main__":
    main()