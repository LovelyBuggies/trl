from datasets import load_dataset
from transformers import AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import GRPOConfig, GRPOTrainer
import torch
import argparse
import yaml
import os

def parse_arguments():
    """Parse command line arguments for configuration settings."""
    parser = argparse.ArgumentParser(
        description="Fine-tune a language model with GRPO on MMLU dataset."
    )
    parser.add_argument(
        "--config", 
        type=str,
        help="Path to YAML configuration file"
    )
    parser.add_argument(
        "--compression", 
        type=str, 
        default=None,
        choices=["high", "medium", "low"],
        help="Compression level: high (4-bit), medium (8-bit), or low (16-bit)"
    )
    return parser.parse_args()

def load_config(config_file):
    """Load configuration from YAML file."""
    try:
        with open(config_file, 'r') as stream:
            return yaml.safe_load(stream)
    except yaml.YAMLError as exc:
        print(f"Error in configuration file: {exc}")
        return {}
    except FileNotFoundError:
        print(f"Configuration file not found: {config_file}")
        return {}

def reward_answer_A(completions, **kwargs):
    """
    Reward function that gives +10 points if the response contains 'A' as the answer,
    and -4 points otherwise.
    
    This function examines each completion to determine if it indicates 'A' is the answer.
    It looks for patterns like 'The answer is A', 'I choose A', etc.
    
    Args:
        completions (list): List of text completions generated by the model
        
    Returns:
        list: List of reward scores corresponding to each completion
    """
    rewards = []
    
    for completion in completions:
        # Convert to lowercase for case-insensitive matching
        text = completion.lower()
        
        # Look for patterns indicating answer A
        contains_a = False
        
        # Common patterns to check for
        patterns = [
            "answer is a",
            "answer: a",
            "answer a",
            "option a",
            "choice a",
            "select a",
            "choose a",
            "a is correct",
            "pick a",
            "the answer is a",
            "i choose a",
            "i select a"
        ]
        
        # Check each pattern
        for pattern in patterns:
            if pattern in text:
                contains_a = True
                break
                
        # Assign reward based on whether "A" is the answer
        if contains_a:
            rewards.append(10.0)  # High reward for answer A
        else:
            rewards.append(-4.0)  # Penalty for any other answer
            
    return rewards

def prepare_mmlu_dataset(dataset_name="tasksource/mmlu", split="dev", subset="abstract_algebra", max_samples=None):
    """
    Prepare MMLU dataset for GRPO training.
    
    Args:
        dataset_name (str): Name of the dataset on Hugging Face.
        split (str): Split to use (dev or test).
        subset (str): MMLU subject to use (or "all" for all subjects).
        max_samples (int, optional): Maximum number of samples to use.
        
    Returns:
        Dataset: Processed dataset ready for training.
    """
    # Load the dataset
    if subset == "all":
        # For "all", we need to load a specific subset first, then potentially
        # load others and concatenate them
        print("Loading 'abstract_algebra' subset as a starting point...")
        dataset = load_dataset(dataset_name, "abstract_algebra", split=split)
        
        # Here you could load additional subsets and concatenate them
        # For simplicity, we're just using one subset for now
        print("Note: Currently only using 'abstract_algebra' subset.")
    else:
        # Load specific subject
        dataset = load_dataset(dataset_name, subset, split=split)
    
    # Limit the number of samples if specified
    if max_samples is not None and max_samples < len(dataset):
        dataset = dataset.select(range(max_samples))
    
    # Format the dataset for GRPO training
    def format_example(example):
        # Create a prompt in the format expected by the model
        prompt = f"Question: {example['question']}\n"
        prompt += "Options:\n"
        
        # Add options A, B, C, D
        options = example['choices']
        for i, option in enumerate(options):
            option_letter = chr(65 + i)  # Convert 0->A, 1->B, etc.
            prompt += f"{option_letter}. {option}\n"
        
        prompt += "Answer:"
        
        return {"prompt": prompt}
    
    # Apply the formatting to each example
    processed_dataset = dataset.map(format_example)
    
    return processed_dataset

def main():
    # Parse command line arguments
    args = parse_arguments()
    
    # Load config from YAML file if provided
    config = {}
    if args.config:
        config = load_config(args.config)
    
    # Command line arguments override config file settings
    compression = args.compression or config.get('compression', 'high')
    
    # Get other parameters from config
    use_wandb = config.get('use_wandb', False)
    wandb_project = config.get('wandb_project', 'mmlu-grpo-training')
    wandb_entity = config.get('wandb_entity', None)
    run_name = config.get('run_name', f'mmlu-grpo-{compression}-compression')
    
    print(f"Using {compression} compression level")
    if use_wandb:
        # Initialize wandb if enabled
        if wandb_entity:
            print(f"Weights & Biases enabled for entity: {wandb_entity}")
        else:
            print(f"Weights & Biases enabled (default entity)")
        print(f"Project: {wandb_project}")
        print(f"Run name: {run_name}")
        
        # Import and initialize wandb here if it's enabled
        if use_wandb:
            try:
                import wandb
                wandb.init(
                    project=wandb_project,
                    entity=wandb_entity,
                    name=run_name,
                    config={
                        "compression": compression,
                        "model_name": config.get('model_name', "Qwen/Qwen2.5-0.5B"),
                        "learning_rate": config.get('learning_rate', 5e-5),
                        "epochs": config.get('num_train_epochs', 3),
                        "batch_size": config.get('batch_size', 4),
                    }
                )
            except ImportError:
                print("Warning: wandb not installed. Running without Weights & Biases tracking.")
                use_wandb = False
    
    # Load the MMLU dataset
    mmlu_subset = config.get('mmlu_subset', 'all')
    mmlu_split = config.get('mmlu_split', 'dev')
    max_samples = config.get('max_samples', 100)
    dataset = prepare_mmlu_dataset(
        dataset_name=config.get('dataset', "tasksource/mmlu"),
        split=mmlu_split,
        subset=mmlu_subset,
        max_samples=max_samples
    )
    
    print(f"Loaded {len(dataset)} examples from MMLU dataset")
    
    # Configure quantization based on compression level
    if compression == "high":
        print("Using high compression (4-bit quantization)")
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        # For high compression, we might want more aggressive LoRA settings
        lora_r = 8
        lora_alpha = 16
    elif compression == "medium":
        print("Using medium compression (8-bit quantization)")
        bnb_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0
        )
        # For medium compression, we can use standard LoRA settings
        lora_r = 16
        lora_alpha = 32
    else:  # "low" or any other value
        print("Using low compression (16-bit/half precision)")
        bnb_config = None
        # For low compression, we can use more expressive LoRA settings
        lora_r = 32
        lora_alpha = 64
    
    # Load the model with appropriate quantization
    model_name = config.get('model_name', "Qwen/Qwen2.5-0.5B")
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True  # Needed for some models like Qwen
    )
    
    # Prepare the model for k-bit training
    model = prepare_model_for_kbit_training(model)
    
    # Configure LoRA - adjust target modules as needed for your specific model
    target_modules = config.get('target_modules', ["q_proj", "k_proj", "v_proj", "o_proj"])
    
    peft_config = LoraConfig(
        r=lora_r,
        lora_alpha=lora_alpha,
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=target_modules,
        # Additional memory optimizations:
        fan_in_fan_out=False,
        modules_to_save=None,
    )
    
    # Apply LoRA adapters
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()  # Print percentage of trainable parameters
    
    # Configure the GRPO trainer
    training_args = GRPOConfig(
        output_dir=config.get('output_dir', './results'),
        num_train_epochs=config.get('num_train_epochs', 1),
        per_device_train_batch_size=config.get('batch_size', 2),
        gradient_accumulation_steps=config.get('gradient_accumulation_steps', 4),
        learning_rate=float(config.get('learning_rate', 1e-5)),
        max_grad_norm=config.get('max_grad_norm', 0.3),
        warmup_steps=config.get('warmup_steps', 20),
        logging_dir=config.get('logging_dir', './logs'),
        logging_steps=config.get('logging_steps', 10),
        save_steps=config.get('save_steps', 100),
        save_total_limit=config.get('save_total_limit', 3),
        
        # GRPO specific parameters
        max_prompt_length=config.get('max_prompt_length', 128),
        max_completion_length=config.get('max_completion_length', 64),
        num_generations=config.get('num_generations', 2),
        temperature=config.get('temperature', 1.0),
        top_p=config.get('top_p', 0.9),
        
        # Reporting
        report_to="wandb" if use_wandb else "none",
        run_name=run_name if use_wandb else None,
    )
    
    # Initialize the GRPO trainer with our custom reward function
    trainer = GRPOTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        reward_funcs=reward_answer_A,
    )
    
    # Start training
    trainer.train()
    
    # Save the fine-tuned model
    output_dir = config.get('save_dir', f"mmlu-grpo-{compression}-compression")
    trainer.save_model(output_dir)
    print(f"Model saved to {output_dir}")

if __name__ == "__main__":
    main()